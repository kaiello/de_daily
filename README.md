# Data Engineering Weekly Work Log

This repository is a collection of the day-to-day tasks, projects, and processes I work on as a **Data Engineer**. It serves as a reference for best practices, tools, workflows, and techniques used in data engineering.

## Table of Contents

- [Overview](#overview)
- [Tools & Technologies](#tools--technologies)
- [Weekly Logs](#weekly-logs)
- [Key Projects](#key-projects)
- [Best Practices](#best-practices)
- [Learning Resources](#learning-resources)
- [Contributing](#contributing)
- [Contact](#contact)

## Overview

As a Data Engineer, I manage, build, and optimize data pipelines, ETL processes, and data architecture. This repository documents my daily activities, providing insight into the practical aspects of data engineering, problem-solving approaches, and the use of various tools.

## Tools & Technologies

In my day-to-day work, I primarily use the following tools and technologies:

- **Data Warehousing**: Databricks, AWS S3
- **Programming Languages**: Python, SQL, Scala
- **ETL Tools**: Apache Spark, Apache Airflow
- **Cloud Platforms**: AWS (S3, Lambda, RDS), GCP, Azure
- **Version Control**: Git, GitHub, GitLab
- **Database Management**: Oracle, MySQL, PostgreSQL
- **Other**: Docker, Terraform, Jenkins, Kafka, etc.

## Weekly Logs

Here, I document the key tasks, challenges, and solutions I encounter on a day-to-day basis. Each log entry contains the date, the key tasks worked on, and links to relevant code or processes.

- **[Log - September 2024](logs/september-2024.md)**: Documenting my work on optimizing ETL processes, managing change data capture (CDC), and working with large-scale databases.
  
- **[Log - August 2024](logs/august-2024.md)**: Focused on data migration from Oracle SQL to AWS S3 and handling complex data partitioning in Databricks.

## Key Projects

I regularly work on a variety of projects related to data engineering, ranging from pipeline optimizations to full-scale data migrations. Below are some key projects that Iâ€™ve worked on:

- **Project 1: Procurement Analytics Pipeline**
  - Description: Developed and optimized a scalable data pipeline for procurement analytics.
  - Tools: Databricks, Apache Spark, AWS S3.
  - Key Challenges: Managing slowly changing dimensions (SCD), ensuring data accuracy in dashboards.

- **Project 2: Data Migration**
  - Description: Migrated data from Oracle SQL to AWS S3 and set up change data capture (CDC) for ongoing syncs.
  - Tools: AWS S3, Python, SQL.

## Best Practices

In this section, I document some of the best practices I've learned over time while working on data engineering tasks:

- **ETL Pipeline Design**:
  - Always plan for scalability and maintainability when building pipelines.
  - Use orchestration tools like Airflow to automate complex workflows.

- **Database Management**:
  - Properly index your database to optimize query performance.
  - Always ensure your tables are partitioned based on the query access patterns.

- **Version Control**:
  - Keep all ETL scripts, database schemas, and infrastructure as code in version control.
  - Use meaningful commit messages and branches (e.g., `feature/new-data-model`, `bugfix/fix-partitioning`).

## Learning Resources

A collection of useful books, articles, tutorials, and courses that I find helpful:

- **Books**:
  - *Designing Data-Intensive Applications* by Martin Kleppmann
  - *The Data Warehouse Toolkit* by Ralph Kimball

- **Online Resources**:
  - Databricks Academy
  - AWS Data Engineering Workshops

## Contributing

If you'd like to contribute or suggest improvements, feel free to submit a pull request or open an issue. Please follow the standard guidelines for contributing.

## Contact

Feel free to reach out if you have any questions or suggestions:

- **Email**: kdataguy@gmail.com
- **LinkedIn**: [Your LinkedIn Profile](https://www.linkedin.com/in/kenneth-aiello-phd/)
"""


